{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.1.0\n",
      "Keras version: 2.0.8\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "print(\"Keras version: {}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pos_tag(word):\n",
    "    return word.split('/')[0]\n",
    "\n",
    "regex = re.compile('[-+]?([0-9]+,)?[0-9]+.?[0-9]*')\n",
    "punctuation = re.compile('['+string.punctuation+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data & create Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:  26438\n",
      "Total number of unique words:  4939\n"
     ]
    }
   ],
   "source": [
    "raw_text = open(\"./data/wonderland.txt\").read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "words = raw_text.split()\n",
    "#words = [remove_pos_tag(word) for word in words if not regex.search(word)]\n",
    "#words = [word for word in words if not punctuation.search(word)]\n",
    "#words = words[:90000]\n",
    "vocab = sorted(list(set(words)))\n",
    "\n",
    "# Lookup tables\n",
    "word_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "int_to_word = dict((i, c) for i, c in enumerate(vocab))\n",
    "\n",
    "print(\"Total number of words: \", len(words))\n",
    "print(\"Total number of unique words: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patterns:  26408\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 30\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(words) - seq_length):\n",
    "    seq_in = words[i:i + seq_length]\n",
    "    seq_out = words[i + seq_length]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "    dataY.append(word_to_int[seq_out])\n",
    "\n",
    "print(\"Total number of patterns: \", len(dataX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she\n",
      "had\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([int_to_word[value] for value in dataX[0]]))\n",
    "print(int_to_word[dataY[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26408, 30, 1)\n",
      "(26408, 4939)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(vocab))\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = keras.utils.to_categorical(dataY, len(vocab))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 30, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4939)              1269323   \n",
      "=================================================================\n",
      "Total params: 1,533,515\n",
      "Trainable params: 1,533,515\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(X.shape[1], X.shape[2]))\n",
    "x = LSTM(256)(inp)\n",
    "x = Dropout(0.2)(x)\n",
    "output = Dense(y.shape[1], activation ='softmax')(x)\n",
    "\n",
    "generative_model = Model(inputs=inp, outputs=output)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "generative_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "generative_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "26408/26408 [==============================] - 16s - loss: 7.4861    \n",
      "Epoch 2/5\n",
      "26408/26408 [==============================] - 15s - loss: 6.9374    \n",
      "Epoch 3/5\n",
      "26408/26408 [==============================] - 15s - loss: 6.8494    \n",
      "Epoch 4/5\n",
      "26408/26408 [==============================] - 14s - loss: 7.6380    \n",
      "Epoch 5/5\n",
      "26408/26408 [==============================] - 15s - loss: 7.5289    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25212501f60>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 2809, 4391, 666, 2649, 622, 4721, 455, 2566, 4244, 4792, 506, 4669, 2397, 2272, 4368, 4391, 2094, 2340, 3649, 586, 2340, 4502, 362, 1093, 192, 2890, 1476, 506, 4754]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "pattern = dataX[1000][:]\n",
    "\n",
    "print(pattern)\n",
    "\n",
    "# generate characters\n",
    "for i in range(15):\n",
    "    x = np.reshape(pattern, (1, seq_length, 1))\n",
    "    x = x / float(len(vocab))\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    #print(result)\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4368, 4391, 2094, 2340, 3649, 586, 2340, 4502, 362, 1093, 192, 2890, 1476, 506, 4754, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244, 4244]\n",
      "time to hear it say, as it turned a corner, 'oh my ears and whiskers, the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(' '.join([remove_pos_tag(int_to_word[value]) for value in pattern]))\n",
    "print(' '.join([remove_pos_tag(int_to_word[value]) for value in generated_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
