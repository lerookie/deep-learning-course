{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.1.0\n",
      "Keras version: 2.0.8\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "print(\"Keras version: {}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_pos_tag(word):\n",
    "    return word.split('/')[0]\n",
    "\n",
    "#regex = re.compile('[-+]?([0-9]+,)?[0-9]+.?[0-9]*')\n",
    "#punctuation = re.compile('['+string.punctuation+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data & create Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'an', 'oct.', '19', 'review', 'of', '``', 'the', 'misanthrope', \"''\", 'at', 'chicago', \"'s\", 'goodman', 'theatre', '(', '``', 'revitalized', 'classics', 'take', 'the', 'stage', 'in', 'windy', 'city', ',', \"''\", 'leisure', '&', 'arts', ')', ',', 'the', 'role', 'of', 'celimene', ',', 'played', 'by', 'kim', 'cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'christina', 'haag', '.'], ['ms.', 'haag', 'plays', 'elianti', '.'], ['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', '1,200', 'cars', 'in', '1990', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', '1,214', 'cars', 'in', 'the', 'u.s.'], ['howard', 'mosher', ',', 'president', 'and', 'chief', 'executive', 'officer', ',', 'said', 'he', 'anticipates', 'growth', 'for', 'the', 'luxury', 'auto', 'maker', 'in', 'britain', 'and', 'europe', ',', 'and', 'in', 'far', 'eastern', 'markets', '.']]\n"
     ]
    }
   ],
   "source": [
    "raw_text = open(\"./data/train.txt\").read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "sentences = []\n",
    "for line in raw_text.splitlines():\n",
    "    sentences.append([remove_pos_tag(token) for token in line.split()])\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=39428, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "model = Word2Vec(sentences, size=embedding_size, min_count=1, workers=4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disbursed', 'asia-pacific', 'lady', 'permanently', 'essence']\n",
      "[-0.07749306 -0.01745089  0.01845957  0.0399294  -0.07870384 -0.04518595\n",
      "  0.06944592 -0.06728533 -0.01565855  0.01094825  0.05835632 -0.06537049\n",
      "  0.03464763 -0.03954552  0.01479189  0.00681547 -0.00546093  0.01643654\n",
      " -0.00593737  0.04881624 -0.03542344  0.03172927  0.0189415   0.09304802\n",
      " -0.08370103 -0.04490628  0.01748649 -0.02752062 -0.00998102 -0.02635884\n",
      " -0.01203324 -0.06892681  0.00340204 -0.00280302 -0.05070677  0.02227013\n",
      " -0.01356892 -0.01441826 -0.04427019  0.07332291  0.00551603 -0.03409106\n",
      " -0.02135161  0.06513803  0.03087234 -0.06501083  0.00810544  0.00127457\n",
      " -0.00891277 -0.03836843  0.00802573  0.03356352  0.04131276 -0.02176528\n",
      "  0.01620766 -0.00201646 -0.00997739 -0.00179663 -0.02330303  0.02618125\n",
      "  0.00066245  0.00201343  0.00206599 -0.0135767  -0.01176804  0.01491551\n",
      " -0.01086763 -0.08314821  0.0270504  -0.02356321 -0.02976044  0.04282209\n",
      " -0.09636673  0.03542997 -0.05935106 -0.02013326  0.02984549 -0.01025761\n",
      "  0.05726708 -0.00934316  0.02964017  0.06760594 -0.01473034  0.05826116\n",
      "  0.02236464  0.01738211 -0.04553561 -0.01660139 -0.04367538 -0.03135599\n",
      "  0.05047631 -0.01665728 -0.03721618 -0.01056586 -0.05449966  0.05051212\n",
      " -0.08028553 -0.02805902 -0.06013623  0.02221895]\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words[:5])\n",
    "print(model['motivated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patterns:  950013\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for sentence in sentences for token in sentence]\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 15\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(tokens) - seq_length):\n",
    "    seq_in = tokens[i:i + seq_length]\n",
    "    seq_out = tokens[i + seq_length]\n",
    "    dataX.append([model[token] for token in seq_in])\n",
    "    dataY.append(model[seq_out])\n",
    "\n",
    "print(\"Total number of patterns: \", len(dataX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950013, 15, 100)\n",
      "(950013, 100)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(dataX)\n",
    "y = np.asarray(dataY)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in an oct. 19 review of `` the misanthrope '' at chicago 's goodman theatre\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([model.wv.similar_by_vector(value)[0][0] for value in dataX[0]]))\n",
    "print(model.wv.similar_by_vector(dataY[0])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950013, 15, 100)\n",
      "(950013, 100)\n",
      "[[-1.94135904  1.18107879 -0.72279423 ...,  1.11281264 -1.27893138\n",
      "   0.77426594]\n",
      " [ 1.25889397  0.96553886  0.84843987 ...,  1.04828274 -0.60708982\n",
      "   1.27152824]\n",
      " [ 1.12125552  0.34008792 -0.35216582 ...,  0.78047252  0.32879883\n",
      "   0.98237455]\n",
      " ..., \n",
      " [-1.80498862  2.31865454  1.19851863 ...,  0.19837078 -1.05653787\n",
      "   2.36574316]\n",
      " [-0.01220683  0.06991564 -0.04197785 ...,  0.06399094 -0.07228622\n",
      "   0.10123127]\n",
      " [-0.02247826  0.04222622 -0.00274354 ...,  0.07011089 -0.02106756\n",
      "   0.10049959]]\n",
      "[-1.05211306  1.29784453  0.46849054  2.89595246 -1.35392296 -0.18713841\n",
      "  0.8473556  -1.123649   -0.28777435 -1.28485274 -1.28018641  1.45036888\n",
      " -0.74645275  0.13564591 -0.67898202  0.54699993 -2.23401189  0.00465996\n",
      "  1.30263579 -0.23561403  1.13053334  2.61760926  0.27111971 -0.78277457\n",
      "  1.0724988   1.25912011  0.63904119  1.84457207 -0.63772315 -1.22739208\n",
      " -1.28988767 -1.93595517 -1.5741235   2.16449213  0.27845252  0.02901214\n",
      " -1.48615122 -1.11524868 -0.27330109  0.08029784 -0.99527681 -2.08079481\n",
      "  1.19905353  0.84464586 -3.23222089 -1.52817464  1.24088776  0.95681459\n",
      "  1.07716143 -2.0580616  -1.18708634  0.5408501   0.7636444  -0.3709411\n",
      "  1.43837368 -1.21496427 -0.95963931  0.74110717 -0.27689326  0.90715814\n",
      " -0.83169276 -0.9357608   1.34506929  0.96558398  2.88358116  2.90301991\n",
      "  1.01415384 -0.83415347  2.28457522 -1.25001132  1.83119619 -0.49707779\n",
      " -0.87477493  0.1985793  -0.48876286  1.71462226 -1.67566502 -0.64258355\n",
      "  0.03648565 -1.22323394  0.40011349 -1.13179553  0.86354458  0.64854932\n",
      "  0.85709369 -0.22770305 -0.18460265  1.05425489 -2.1855588  -0.69083577\n",
      "  2.24116135  1.0671252  -1.81809199  0.27911153  0.60806108 -1.18037963\n",
      "  1.19929242  1.42017388  2.39415097  2.12480497]\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (len(dataX), seq_length, embedding_size))\n",
    "print(X.shape)\n",
    "\n",
    "y = np.reshape(dataY, (len(dataY), embedding_size))\n",
    "print(y.shape)\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(X.shape[1], X.shape[2]))\n",
    "x = LSTM(256)(inp)\n",
    "x = Dropout(0.2)(x)\n",
    "output = Dense(y.shape[1], activation ='softmax')(x)\n",
    "\n",
    "generative_model = Model(inputs=inp, outputs=output)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "generative_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "generative_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.fit(X, y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_text = []\n",
    "pattern = dataX[1000][:]\n",
    "\n",
    "print(pattern)\n",
    "\n",
    "# generate characters\n",
    "for i in range(15):\n",
    "    x = np.reshape(pattern, (1, seq_length, 1))\n",
    "    x = x / float(len(vocab))\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    #print(result)\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pattern)\n",
    "print(' '.join([remove_pos_tag(int_to_word[value]) for value in pattern]))\n",
    "print(' '.join([remove_pos_tag(int_to_word[value]) for value in generated_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to implement temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "generated_text = []\n",
    "pattern = dataX[2000][:]\n",
    "\n",
    "# generate characters\n",
    "for i in range(15):\n",
    "    x = np.reshape(pattern, (1, seq_length, 1))\n",
    "    x = x / float(len(vocab))\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = sample(prediction[0])\n",
    "    result = int_to_word[index]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(pattern)\n",
    "print(' '.join([remove_pos_tag(int_to_word[value]) for value in pattern]))\n",
    "print(' '.join([remove_pos_tag(int_to_word[value]) for value in generated_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
